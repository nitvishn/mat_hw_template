\documentclass{article}
\usepackage[utf8]{inputenc}
\usepackage[margin=0.75in]{geometry}
\usepackage{enumerate}
\usepackage{amsmath}
\usepackage{amsfonts} 
\usepackage{amssymb}
\usepackage{amsthm}
\usepackage{mathtools}
\usepackage{float}
\usepackage{array}
\usepackage{makecell}
\DeclarePairedDelimiter{\ceil}{\lceil}{\rceil}

\renewcommand\theadalign{bc}
\renewcommand\theadfont{\bfseries}
\renewcommand\theadgape{\Gape[4pt]}
\renewcommand\cellgape{\Gape[4pt]}

\newcommand{\N}{\mathbb{N}}
\newcommand{\Z}{\mathbb{Z}}
\newcommand{\Q}{\mathbb{Q}}
\newcommand{\C}{\mathbb{C}}
\newcommand{\R}{\mathbb{R}}
\newcommand{\F}{\mathbb{F}}
\newtheorem{theorem}{Theorem}
\newtheorem{corollary}{Corollary}[theorem]
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem*{remark}{Remark}
\newcommand{\cdotscalar}{\;\widetilde{\cdot}\;}
\newcommand{\vectorplus}{\;\widetilde{+}\;}

\newcommand\widebar[1]{\mathop{\overline{#1}}}
\newcommand*\conj[1]{\widebar{#1}}

\begin{document}

\section{}

\begin{lemma}
    \label{lemma:num_vecs}
    The number of vectors contained in a $k$-dimensional linear subspace of $\F_p^n$ is equal to $p^k$.
\end{lemma}

\begin{proof}
    Let $V$ be a $k$-dimensional linear subspace of $\F_p^n$. Since $V$ is $k$-dimensional, it is spanned by a basis of length $k$. Let us call this basis $B = (v_1, v_2, \dots, v_k)$.

    We will now show that every linear combination of the vectors in $B$ is a unique vector in $V$. We will show this by demonstrating that if a vector has two representations as a linear combination of $v_1, \dots, v_k$, then those representations are equal. Observe the following:

    \begin{align*}
        a_1 v_1 + a_2 v_2 + \dots + a_k v_k                         & = b_1 v_1 + b_2 v_2 + \dots + b_k v_k \\
        (a_1 - b_1) v_1 + (a_2 - b_2) v_2 + \dots + (a_k - b_k) v_k & = 0                                   \\
    \end{align*}

    Since $B$ is linearly independent, all scalar multiples of $v_1, \dots, v_k$ must be trivial if their linear combination is equal to zero. Therefore:

    \begin{align*}
        a_1 & = b_1 \\
        a_2 & = b_2 \\
        \dots       \\
        a_k & = b_k
    \end{align*}

    All vectors in $V$ must be represented as some linear combination of the vectors in the basis, and all these vectors can be only represented as one linear combination, there are as many vectors as there are possible linear combinations of $v_1, \dots, v_k$.

    The number of distinct $(a_1, a_2, \dots, a_k)$ are given by the number of choices for each of $a_1, \dots, a_k$. Since there are $p$ elements in the field $\F_p$, and there are $k$ scalars, the total number of possible linear combinations is \[\underbrace{p \cdot \ldots \cdot p}_{k \text{ times}} = p^k\]
\end{proof}

\begin{lemma}
    \label{lemma:num_lists}
    The number of linearly independent lists of length $\ell$ in a linear subspace $V$ of $\F_p^n$ of dimension $d$ is given by

    \[\prod _{i = 1} ^{\ell} p^d - p^{i - 1}\]
\end{lemma}

\begin{proof}
    We can calculate the number of linearly independent lists of length $\ell$ according to the following set of rules.

    \begin{enumerate}
        \item Pick $v_1$ to be some non-zero vector in $V$. Since there are $p^d$ vectors in $V$ (Lemma \ref{lemma:num_vecs}) and one of them is $\vec{0}$, there are $p^d - 1$ choices for $v_1$.
        \item We must choose $v_2$ such that it is not contained in the span of $v_1$. Since we know that $\text{Span}(v_1)$ is a one dimensional linear subspace of $\F_p^n$, we know there are $p$ vectors contained in $\text{Span}(v_1)$. This includes the zero vector. Therefore, there are $p^d - p$ choices for $v_2$.
        \item After having chosen the $i$th vector in the list, we know that $\text{Span}(v_1, \dots, v_i)$ is a $i$ dimensional linear subspace of $\F_p^n$. There are $p^i$ vectors in this subspace, so we have $p^d - p^i$ choices for $v_{i+1}$.
    \end{enumerate}

    Therefore, there are $p^d - p^{i-1}$ choices for each vector $v_i$ in the list. The total number of linearly independent lists can be calculated by multiplying them together as follows:

    \begin{align*}
        (p^d - p^0) \cdot (p^d - p^1) \cdot (p^d - p^2) \cdot \ldots \cdot (p^d - p^{l-1}) \\
        = \prod _{i = 1} ^{\ell} p^d - p^{i - 1}
    \end{align*}
\end{proof}

Now, we will find a general formula for the number of $k$-dimensional linear subspaces of $\F_p^n$ for all $k$. Any $k$-dimensional linear subspace has a basis $(v_1, \dots, v_k)$ which is linearly independent. We will now calculate the number of possible bases a $k$-dimensional linear subspace $S$ may have.

This is equivalent to counting the number of $k$-length lists with vectors in $S$ that are linearly independent. Since the length of any linearly independent list is less than or equal to the length of any spanning list, and we know that there exists a spanning list of length $k$ that spans $S$ (since it is $k$-dimensional), all linearly independent lists of length $k$ must be spanning lists. If not, then it would be possible to extend the linearly independent list while still preserving independence, producing a $k + 1$ length linearly independent list which is greater than the length of the spanning list.

The number of $k$-length linearly independent lists in a linear subspace of dimension $k$ is equal to $\prod _{i = 1} ^{k} p^k - p^{i - 1}$ (Lemma $\ref{lemma:num_lists}$). This refers to the number of representations a linear subspace could have in terms of distinct lists of vectors that span the space.

Now, let us consider all possible $k$-length linearly independent lists of vectors in $\F_p^n$. Each of these lists can be mapped to a $k$-dimensional linear subspace that it spans. From Lemma $\ref{lemma:num_lists}$, there are $\prod _{i = 1} ^{k} p^n - p^{i - 1}$ such lists.

Therefore, the number of linear subspaces of dimension $k$ in $\F_p^n$ is equal to the total number of possible bases for $k$-dimensional linear subspaces divided by the number of equivalent bases that span the same linear subspace. This is equal to \[\prod_{i=1}^k \; \frac{p^n - p^{i - 1}}{p^k - p^{i-1}}\]

\newpage
\section{}

Below, we list the set of points in $\F_2^3$ in the order of their labelling:

\begin{enumerate}
    \item $\{(0, 0, 0), (0, 1, 0)\}$
    \item $\{(0, 0, 0), (1, 0, 0)\}$
    \item $\{(0, 0, 0), (1, 1, 0)\}$
    \item $\{(0, 0, 0), (0, 0, 1)\}$
    \item $\{(0, 0, 0), (0, 1, 1)\}$
    \item $\{(0, 0, 0), (1, 0, 1)\}$
    \item $\{(0, 0, 0), (1, 1, 1)\}$
\end{enumerate}

Next, we list the set of lines in $\F_2^3$ in the order of their labelling:

\begin{enumerate}
    \item $\{(0, 0, 0), (0, 1, 0), (1, 0, 0), (1, 1, 0)\}$
    \item $\{(0, 0, 0), (0, 0, 1), (0, 1, 0), (0, 1, 1)\}$
    \item $\{(0, 0, 0), (0, 1, 0), (1, 0, 1), (1, 1, 1)\}$
    \item $\{(0, 0, 0), (0, 0, 1), (1, 0, 0), (1, 0, 1)\}$
    \item $\{(0, 0, 0), (0, 1, 1), (1, 0, 0), (1, 1, 1)\}$
    \item $\{(0, 0, 0), (0, 0, 1), (1, 1, 0), (1, 1, 1)\}$
    \item $\{(0, 0, 0), (0, 1, 1), (1, 0, 1), (1, 1, 0)\}$
\end{enumerate}

The incidence matrix is as follows:

\[
    M = \begin{bmatrix}
        1 & 1 & 1 & 0 & 0 & 0 & 0 \\
        1 & 0 & 0 & 1 & 1 & 0 & 0 \\
        1 & 0 & 0 & 0 & 0 & 1 & 1 \\
        0 & 1 & 0 & 1 & 0 & 1 & 0 \\
        0 & 1 & 0 & 0 & 1 & 0 & 1 \\
        0 & 0 & 1 & 1 & 0 & 0 & 1 \\
        0 & 0 & 1 & 0 & 1 & 1 & 0 \\
    \end{bmatrix}
\]

We notice that there are three ones in each column, denoting that there are three points incident to each line. We may explain this using Lemma $\ref{lemma:num_vecs}$ from Question 1. There are $2^2$ vectors contained in each 2 dimensional subspace of $\F^3_2$, and $3$ of them are non-zero. Each of these three vectors are pairwise linearly independent, since the only scalar multiples of a vector are itself and the zero vector (since we may only scale by 0 and 1). Therefore, if $v_1$, $v_2$, and $v_3$ are contained in $S$, a two-dimensional subspace, then $\text{Span}(v_1)$, $\text{Span}(v_2)$, and $\text{Span}(v_3)$ are distinct subspaces of $S$. There exist no more 1-dimensional subspaces of $S$ as it contains only three non-zero vectors.

We notice that there are three ones in each row, denoting that there are three lines incident to each point. Consider a point $p$ generated by the span of a vector $v_1$. Now, let us observe how many distinct subspaces we may generate that contain $p$. First, we count the number of linearly independent lists we can generate including $v_1$. Since the span of $v_1$ contains $p$ vectors, and the space contains $p^3$ vectors, we have $p^3 - p = 8 - 2 = 6$ choices for $v_2$. Now, we count how many bases (that include $v_1$) a subspace containing $v_1$ has. Since each subspace has $p^2 - 1 = 3$ nonzero vectors, and we have already picked one of the vectors in our basis to be $v_1$, we have $2$ choices for $v_2$. Therefore, the number of 2-dimensional subspaces containing $v_1$ equals the number of total bases in which $v_1$ is present divided by the number of bases that span the same subspace. This is equal to $6/2 = 3$.


\newpage
\section{}

We will proceed using Gaussian elimination. At each step of the elimination, we will maintain and update a series of expressions that represent each row in terms of the initial vectors $v_1, v_2, v_3, v_4$. We will use the notation $R_i \to aR_j + bR_k$ to represent re-assigning row $i$ to $a$ times row $j$ plus $b$ times row $k$. The basis we will choose for the Gaussian elimination is the standard basis $e_1, e_2, e_3, e_4$. We will begin with the following matrix:

\[
    \begin{bmatrix}
        2 & 3 & 0 & 0  \\
        0 & 0 & 1 & -1 \\
        1 & 0 & 0 & 4  \\
        0 & 0 & 0 & 2
    \end{bmatrix} = \begin{pmatrix}
        v_1 \\v_2\\v_3\\v_4
    \end{pmatrix}\\
\]

\begin{enumerate}[(Step 1): ]
    \item $R_1 \to R_3, R_2 \to R_1, R_3 \to R_2$
          \[ \begin{bmatrix}
                  1 & 0 & 0 & 4  \\
                  2 & 3 & 0 & 0  \\
                  0 & 0 & 1 & -1 \\
                  0 & 0 & 0 & 2
              \end{bmatrix} = \begin{pmatrix}
                  v_3 \\
                  v_1 \\
                  v_2 \\
                  v_4
              \end{pmatrix}\\ \]
    \item $R_2 \to R_2 - 2R_1$
          \[ \begin{bmatrix}
                  1 & 0 & 0 & 4  \\
                  0 & 3 & 0 & -8 \\
                  0 & 0 & 1 & -1 \\
                  0 & 0 & 0 & 2
              \end{bmatrix} = \begin{pmatrix}
                  v_3        \\
                  v_1 - 2v_3 \\
                  v_2        \\
                  v_4
              \end{pmatrix}\\ \]
    \item $R_2 \to R_2/3$
          \[ \begin{bmatrix}
                  1 & 0 & 0 & 4    \\
                  0 & 1 & 0 & -8/3 \\
                  0 & 0 & 1 & -1   \\
                  0 & 0 & 0 & 2
              \end{bmatrix} = \begin{pmatrix}
                  v_3            \\
                  v_1/3 - 2v_3/3 \\
                  v_2            \\
                  v_4
              \end{pmatrix}\\ \]
    \item $R_4 \to R_4/2$
          \[ \begin{bmatrix}
                  1 & 0 & 0 & 4    \\
                  0 & 1 & 0 & -8/3 \\
                  0 & 0 & 1 & -1   \\
                  0 & 0 & 0 & 1
              \end{bmatrix} = \begin{pmatrix}
                  v_3            \\
                  v_1/3 - 2v_3/3 \\
                  v_2            \\
                  v_4/2
              \end{pmatrix}\\ \]
    \item $R_3 \to R_3+R_4, R_2 \to R_2 + \frac{8}{3}R_4, R_1 \to R_1 - 4R_4$
          \[ \begin{bmatrix}
                  1 & 0 & 0 & 0 \\
                  0 & 1 & 0 & 0 \\
                  0 & 0 & 1 & 0 \\
                  0 & 0 & 0 & 1
              \end{bmatrix} = \begin{pmatrix}
                  v_3 - 2v_4              \\
                  v_1/3 - 2v_3/3 + 4v_4/3 \\
                  v_2 + v_4/2             \\
                  v_4/2
              \end{pmatrix}\\ \]
\end{enumerate}

We have reduced our matrix into Reduced Row Echelon Form to obtain the list of final vectors as $e_1, e_2, e_3, e_4$. Since we know that they span $R^4$, and that their span is the same as that of $(v_1, v_2, v_3, v_4)$, we know that $(v_1, v_2, v_3, v_4)$ spans $R^4$.

If $(v_1, v_2, v_3, v_4)$ were not linearly independent, then it would be possible to obtain a spanning list of length $3$ that spans $R^4$ by removing a vector that is contained in the span of the previous sub-list. However, this would imply that any basis for $R^4$ has less than length $4$, which is false. Therefore, $(v_1, v_2, v_3, v_4)$ is linearly independent and spanning, and hence, is a basis for $R^4$.

Since we maintained a record of the operations performed on the initial set of vectors, we have expressions for the rows of the matrix $(e_1, e_2, e_3, e_4)$ in terms of $v_1, v_2, v_3$, and $v_4$. Note the following:

\begin{align*}
    e_1 & = v_3 - 2v_4                                                                                \\
        & = 0 \cdot v_1 + 0 \cdot v_2 + 1 \cdot v_3 - 2 \cdot v_4                                     \\
        & = (0, 0, 1, -2) \quad \text{with basis } (v_1, v_2, v_3, v_4)                               \\[1em]
    e_2 & = \frac{1}{3} \cdot v_1 + 0 \cdot v_2 - \frac{2}{3} \cdot v_3 + \frac{4}{3}v_4              \\
        & = \left(2, 0, \frac{2}{3}, \frac{4}{3}\right) \quad \text{with basis } (v_1, v_2, v_3, v_4) \\[1em]
    e_3 & = 0 \cdot v_1 + 1 \cdot v_2 + 0 \cdot v_3 + \frac{1}{2}v_4                                  \\
        & = \left(0, 1, 0, \frac{1}{2}\right) \quad \text{with basis } (v_1, v_2, v_3, v_4)           \\[1em]
    e_4 & = 0 \cdot v_1 + 0 \cdot v_2 + 0 \cdot v_3 + \frac{1}{2}v_4                                  \\
        & = \left(0, 0, 0, \frac{1}{2}\right) \quad \text{with basis } (v_1, v_2, v_3, v_4)           \\[1em]
\end{align*}

\newpage
\section{}

We claim that the following list is a basis for $U$:

\[\left((2, 1, 0, 0, 0), (0, 0, 1, 0, 0), (0, 0, 0, 7, 1) \right)\]

\textbf{Showing that the basis spans $U$}

Consider any vector $(x_1, x_2, x_3, x_4, x_5)$ in $U$. Take the following linear combination:

\begin{align*}
     & \phantom{= \quad} x_2 \cdot (2, 1, 0, 0, 0) + x_3 \cdot (0, 0, 1, 0, 0) + x_5 (0, 0, 0, 7, 1) \\
     & = (2x_2, x_2, 0, 0, 0) + (0, 0, x_3, 0, 0) + (0, 0, 0, 7x_5, x_5)                             \\
     & = (x_1, x_2, 0, 0, 0) + (0, 0, x_3, 0, 0) + (0, 0, 0, x_4, x_5)                               \\
     & = (x_1, x_2, x_3, x_4, x_5)                                                                   \\
\end{align*}

Therefore any vector in $U$ can be achieved by a linear combination of the given vectors.

Note that it is not possible to produce a vector that is not in $U$ by linear combinations of vectors from this basis. This is because all vectors in the basis $(2, 1, 0, 0, 0), (0, 0, 1, 0, 0), (0, 0, 0, 7, 1)$ belong to $U$, and that linear combination of vectors $a u + b v$ where $u = (u_1, u_2, u_3, u_4, u_5), v = (v_1, v_2, v_3, v_4, v_5) \in U$ produces a vector in $U$. This can be verified as follows:

\begin{align*}
     & \phantom{=\quad} a \cdot (u_1, u_2, u_3, u_4, u_5) + b \cdot (v_1, v_2, v_3, v_4, v_5) \\
     & = (au_1, au_2, au_3, au_4, au_5) + (bv_1, bv_2, bv_3, bv_4, bv_5)                      \\
     & = (au_1 + bv_1, au_2 + bv_2, au_3 +  bv_3, au_4 + bv_4, au_5 + bv_5)                   \\
     & = (a(2u_2) + b(2v_2), au_2 + bv_2, au_3 +  bv_3, a(7u_5) + b(7v_5), au_5 + bv_5)       \\
     & = (2(au_2 + bv_2), au_2 + bv_2, au_3 +  bv_3, 7(au_5 + bv_5), au_5 + bv_5)             \\
     & \implies au + bv \in U
\end{align*}

\textbf{Linear Independence}

Consider a linear combination of the vectors $(2, 1, 0, 0, 0), (0, 0, 1, 0, 0), (0, 0, 0, 7, 1)$ being equal to the zero vector:

\begin{align*}
    \vec{0}         & = a_1 \cdot (2, 1, 0, 0, 0) + a_2 \cdot (0, 0, 1, 0, 0) + a_3 (0, 0, 0, 7, 1) \\
                    & = (2a_1, a_1, 0, 0, 0) + (0, 0, a_2, 0, 0) + (0, 0, 0, 7a_3, a_3)             \\
                    & = (2a_1, a_1, a_2, 7a_3, a_3)                                                 \\
    (0, 0, 0, 0, 0) & = (2a_1, a_1, a_2, 7a_3, a_3)                                                 \\[1em]
                    & \implies a_1 = a_2 = a_3 = 0
\end{align*}

Therefore the list of vectors $((2, 1, 0, 0, 0), (0, 0, 1, 0, 0), (0, 0, 0, 7, 1))$ both spans $U$ and is linearly independent, and so is a basis for $U$.

\newpage
\section{}

The equations

\begin{align*}
    2x_1 - 3x^2 + 6x_3 + 2x^4 - 5x^5  \\
    2x_1 - 2x^2 + 2x_3 + 3x^4 - 5x^5  \\
    -2x_1 + 2x^2 - 2x_3 - 4x^4 + 8x^5 \\
    5x_1 - 6x^2 + 9x_3 + 7x^4 - 14x^5 \\
\end{align*}

each represent functions in the dual space of $\R^5$. In the dual space, if $f(x_1, \dots, x_n) = 0$ and $g(x_1, \dots, x_n) = 0$, then $(a\cdot f+b\cdot g)(x_1, \dots, x_n) = a \cdot f(x_1, \dots, x_n) + b \cdot f(x_1, \dots, x_n) = a \cdot 0 + b \cdot 0 = 0$, since $f$ and $g$ are linear functions. So, we may perform Gaussian elimination on these functions and take non-zero linear combinations of them to obtain a set of functions that are straightforward to solve.

A basis for the dual space of $\R^5$ that we will use are the following functions:

\begin{align*}
    x_1 + 0 x^2 + 0 x_3 + 0 x^4 + 0x^5 \\
    0x_1 + x^2 + 0 x_3 + 0 x^4 + 0x^5  \\
    0x_1 + 0x^2 +  x_3 + 0 x^4 + 0x^5  \\
    0x_1 + 0x^2 + 0 x_3 + x^4 + 0x^5   \\
    0x_1 + 0x^2 + 0 x_3 + 0x^4 + x^5   \\
\end{align*}

Therefore the initial matrix for Gaussian elimination is
\[
    \begin{bmatrix}
        2  & -3 & 6  & 2  & -5  \\
        2  & -2 & 2  & 3  & -5  \\
        -2 & 2  & -2 & -4 & 8   \\
        5  & -6 & 9  & 7  & -14
    \end{bmatrix}
\]

We will now perform Gaussian Elimination. We will use the notation $R_i \to a R_i$ to represent scaling Row $i$ by $a \in \R$. We use $R_i \to R_j$ to indicate reassigning Row $i$ to contain Row $j$, and we use $R_i \to R_i + aR_j$ to indicate a shear by $a \cdot R_j$.

\begin{enumerate}[(Step 1)]
    \item $R_1 \to R_1/2$
          \[
              \begin{bmatrix}
                  1  & -3/2 & 3  & 1  & -5/2 \\
                  2  & -2   & 2  & 3  & -5   \\
                  -2 & 2    & -2 & -4 & 8    \\
                  5  & -6   & 9  & 7  & -14
              \end{bmatrix}
          \]
    \item $R_2 \to R_2 - 2R_1,  R_3 \to R_3 + 2R_1, R_4 \to R_4 - 5R_1$
          \[
              \begin{bmatrix}
                  1 & -3/2 & 3  & 1  & -5/2 \\
                  0 & 1    & -4 & 1  & 0    \\
                  0 & -1   & 4  & -2 & 3    \\
                  0 & 1.5  & -6 & 2  & -1.5
              \end{bmatrix}
          \]
    \item $R_3 \to R_3 + R_2, R_4 \to R_4 - \frac{3}{2}R_2$
          \[
              \begin{bmatrix}
                  1 & -3/2 & 3  & 1   & -5/2 \\
                  0 & 1    & -4 & 1   & 0    \\
                  0 & 0    & 0  & -1  & 3    \\
                  0 & 0    & 0  & 0.5 & -1.5
              \end{bmatrix}
          \]
    \item $R_3 \to -R_3$
          \[
              \begin{bmatrix}
                  1 & -3/2 & 3  & 1   & -5/2 \\
                  0 & 1    & -4 & 1   & 0    \\
                  0 & 0    & 0  & 1   & -3   \\
                  0 & 0    & 0  & 0.5 & -1.5
              \end{bmatrix}
          \]
    \item $R_4 \to 2R_4-R_3$
          \[
              \begin{bmatrix}
                  1 & -3/2 & 3  & 1 & -5/2 \\
                  0 & 1    & -4 & 1 & 0    \\
                  0 & 0    & 0  & 1 & -3   \\
                  0 & 0    & 0  & 0 & 0
              \end{bmatrix}
          \]
    \item $R_2 \to R_2-R_3, R_1 \to R_1 - R_3$
          \[
              \begin{bmatrix}
                  1 & -3/2 & 3  & 0 & 1/2 \\
                  0 & 1    & -4 & 0 & 3   \\
                  0 & 0    & 0  & 1 & -3  \\
                  0 & 0    & 0  & 0 & 0
              \end{bmatrix}
          \]
    \item $R_1 \to R_1+ \frac{3}{2}R_2$
          \[
              \begin{bmatrix}
                  1 & 0 & -3 & 0 & 5  \\
                  0 & 1 & -4 & 0 & 3  \\
                  0 & 0 & 0  & 1 & -3 \\
                  0 & 0 & 0  & 0 & 0
              \end{bmatrix}
          \]
\end{enumerate}

Therefore a solution for this system may be obtained from the following equations:

\begin{align*}
    x_1 - 3x_3 + 5x_5 & = 0 \\
    x_2 - 4x_3 + 3x_5 & = 0 \\
    x_4 - 3x_5        & = 0
\end{align*}

We may write $x_1, x_2, x_4$ as follows:


\begin{align*}
    x_1 & = 3x_3 - 5x_5 \\
    x_2 & = 4x_3 - 3x_5 \\
    x_4 & = 3x_5
\end{align*}

Therefore the solutions to this vector space are all vectors of the form \[(3x_3 - 5x_5, 4x_3 - 3x_5, x_3, 3x_5, x_5)\]

If we set $x_3 = \alpha$ and $x_5 = \beta$, then:

\begin{align*}
    (3\alpha - 5\beta, 4\alpha - 3\beta, \alpha, 3\beta, \beta) & = (3\alpha, 4\alpha, \alpha, 0, 0) + (- 5\beta, -3\beta, 0, 3\beta, \beta) \\
                                                                 & = \alpha (3, 4, 1, 0, 0) + \beta (-5, -3, 0, 3, 1)
\end{align*}

Therefore all solutions can be written in terms of linear combinations of $(3, 4, 1, 0, 0)$ and $(-5, -3, 0, 3, 1)$. Therefore, the solutions to these linear equations over $\R$ are:

\[(x_1, x_2, x_3, x_4, x_5) \in \text{Span}((3, 4, 1, 0, 0), (-5, -3, 0, 3, 1))\]

which is a 2-dimensional subspace of $\R^5$.

\newpage
\section{}

We will solve the given system of equations

\begin{align*}
    4x^1 + 2x^2 + 3x^3 & = 2 \\
    -x^1 + 2x^2 - 3x^3 & = 1 \\
    -3x^1 + 2x^2 + x^3 & = 3 \\
\end{align*}

over the field $\F_5$ using the dual space of $\F_5^4$. These equations are special cases of the linear maps

\begin{align*}
    4x^1 + 2x^2 + 3x^3 - 2x^4 & = 0 \\
    -x^1 + 2x^2 - 3x^3 - x^4  & = 0 \\
    -3x^1 + 2x^2 + x^3 - 3x^4 & = 0 \\
\end{align*}

in the dual space of $\F_5^4$, where $x^4 = 1$. As shown in Q5, a solution to any linear combination of functions in the dual space is a solution to the original system of functions, and vice versa. We will perform Gaussian Elimination over these functions using the basis $(x_1, x_2, x_3, x_4)$.

\begin{enumerate}
    \item $R_1 \to R_1 / 4$
          \[
              \begin{bmatrix}
                  1  & 3 & 2  & 2  \\
                  -1 & 2 & -3 & -1 \\
                  -3 & 2 & 1  & -3 \\
              \end{bmatrix}
          \]
    \item $R_2 \to R_1 + R_2$
          \[
              \begin{bmatrix}
                  1  & 3 & 2 & 2  \\
                  0  & 0 & 4 & 1  \\
                  -3 & 2 & 1 & -3 \\
              \end{bmatrix}
          \]
    \item $R_3 \to R_3 + 3R_1$
          \[
              \begin{bmatrix}
                  1 & 3 & 2 & 2 \\
                  0 & 0 & 4 & 1 \\
                  0 & 1 & 2 & 3 \\
              \end{bmatrix}
          \]
    \item $R_3 \to R_2, R_2 \to R_3$
          \[
              \begin{bmatrix}
                  1 & 3 & 2 & 2 \\
                  0 & 1 & 2 & 3 \\
                  0 & 0 & 4 & 1 \\
              \end{bmatrix}
          \]
    \item $R_3 \to R_3/4$
          \[
              \begin{bmatrix}
                  1 & 3 & 2 & 2 \\
                  0 & 1 & 2 & 3 \\
                  0 & 0 & 1 & 4 \\
              \end{bmatrix}
          \]
    \item $R_2 \to R_2 + 3R_3$
          \[
              \begin{bmatrix}
                  1 & 3 & 2 & 2 \\
                  0 & 1 & 0 & 0 \\
                  0 & 0 & 1 & 4 \\
              \end{bmatrix}
          \]
    \item $R_1 \to R_1 + 3R_3$
          \[
              \begin{bmatrix}
                  1 & 3 & 0 & 4 \\
                  0 & 1 & 0 & 0 \\
                  0 & 0 & 1 & 4 \\
              \end{bmatrix}
          \]
    \item $R_1 \to R_1 + 2R_2$
          \[
              \begin{bmatrix}
                  1 & 0 & 0 & 4 \\
                  0 & 1 & 0 & 0 \\
                  0 & 0 & 1 & 4 \\
              \end{bmatrix}
          \]
\end{enumerate}

This final matrix is in Reduced Row Echelon Form. Let us extract the functions from the matrix: 

\begin{align*}
    x_1 + 4x_4 &= 0\\
    x_2 &= 0\\
    x_3 + 4x_4 &= 0
\end{align*}

Since we are examining the special case in which $x_4 = 1$, let us simplify further: 

\begin{align*}
    x_1 &= - 4 = 1\\
    x_2 &= 0\\
    x_3 &= -4 = 1
\end{align*}

Since there are no free variables, the solution $x_1 = 1$, $x_2 = 0$, $x_3 = 1$ is the only solution over the field $\F_5$ to the given system of equations. 

\end{document}